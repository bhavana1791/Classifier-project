# -*- coding: utf-8 -*-
"""Copy of sentiment_neural

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zFr4wen1BlIJAQSzM3TYu4N1sXhW2KZ9
"""

import pandas as pd
import numpy as np
import argparse
import sys
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchtext
import collection
from collections import Counter
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder
from keras.preprocessing.sequence import pad_sequences
import torchvision
import torchvision.transforms as transforms
le = LabelEncoder()

def train_test(filename):

    df = pd.read_csv(filename)
    train , test = train_test_split(
    df,
    test_size = 0.20
)
    return train , test

class SentiLSTM(nn.Module):
    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob = 0.5):
        super(SentiLSTM, self).__init__()
        self.output_size = output_size
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first = True)
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(hidden_dim, output_size) # fully connected
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x, hidden):
        batch_size = x.size(0)
        x = x.long() # cast to long tensor
        embeds = self.embedding(x)
        lstm_out, hidden = self.lstm(embeds, hidden)
        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)
        
        out = self.dropout(lstm_out)
        out = self.fc(out)
        out = self.sigmoid(out)
        
        out = out.view(batch_size, -1)
        out = out[:,-1]
        return out, hidden
    
    def init_hidden(self, batch_size):
        weight = next(self.parameters()).data
        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),
                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))
        return hidden

if __name__ == "__main__":
   argparser = argparse.ArgumentParser(description='Sentiment analysis neural model')
   argparser.add_argument('input_file', action="store", help='log file for training')
   args = argparser.parse_args()
   train , test = train_test(args.input_file)
   device = 'cuda' if torch.cuda.is_available() else 'cpu'
   print(f'Using {device} device')
   target = train.labels
   train_target = torch.tensor(target.values.astype(np.float32))
   train_review = torch.tensor(train.drop('labels', axis = 1).apply(le.fit_transform).values.astype(np.float32)) 
   train_tensor = torch.utils.data.TensorDataset(train_review, train_target)
   trainloader = torch.utils.data.DataLoader(train_tensor, batch_size= 512,
                                          shuffle=True, num_workers=2 , drop_last = True)
   target = test.labels
   test_target = torch.tensor(target.values.astype(np.float32))
   test_review = torch.tensor(test.drop('labels', axis = 1).apply(le.fit_transform).values.astype(np.float32)) 
   test_tensor = torch.utils.data.TensorDataset(test_review, test_target) 
   testloader = torch.utils.data.DataLoader(test_tensor, batch_size= 512,
                                         shuffle= True, num_workers=2 , drop_last=True)
   vocab_size = len(train_tensor)
   output_size = 1
   embedding_dim = 400 
   hidden_dim = 1024
   n_layers = 2
   batch_size = 512

   model = SentiLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)
   model.to(device)
   print(model)
   lr = 0.003
   criterion = nn.BCELoss() 
   optimizer = torch.optim.Adam(model.parameters(), lr = lr)
   epochs = 2
   counter = 0
   loop_print = 200
   clip = 5
   test_loss_min = np.Inf

   model.train() 

   for i in range(epochs):
     h = model.init_hidden(batch_size)
    
     for inputs, labels in trainloader:
        counter += 1
        inputs, labels = inputs.to(device), labels.to(device)
        h = tuple([e.data for e in h])
        model.zero_grad() 
        output, h = model(inputs, h) 
        loss = criterion(output.squeeze(), labels.float()) 
        loss.backward() 
        nn.utils.clip_grad_norm_(model.parameters(), clip) 
        optimizer.step() 
        
        # METRICS:
        if counter%loop_print == 0:
            test_h = model.init_hidden(batch_size)
            test_losses = []
            num_correct = 0
            model.eval()
            for inp, lab in testloader:
                test_h = tuple([each.data for each in test_h])
                inp, lab = inp.to(device), lab.to(device)
                out, test_h = model(inp, test_h)
                test_loss = criterion(out.squeeze(), lab.float())
                test_losses.append(test_loss.item())

                # Computing accuracy
                pred = torch.round(out.squeeze()) 
                correct_tensor = pred.eq(lab.float().view_as(pred)) 
                correct = np.squeeze(correct_tensor.cpu().numpy()) 
                num_correct += np.sum(correct) 

            print("Test loss: {:.3f}".format(np.mean(test_losses)))
            test_acc = num_correct/len(testloader.dataset)
            print("Test accuracy: {:.3f}%".format(test_acc*100))

                
            model.train()
            print("Epoch: {}/{}...".format(i+1, epochs),
                  "Step: {}...".format(counter),
                  "Loss: {:.6f}...".format(loss.item()),
                  "Test Loss: {:.6f}".format(np.mean(test_losses)))
            if np.mean(test_losses) <= test_loss_min:
                torch.save(model.state_dict(), '/content/gdrive/My Drive/BILTZai/Sentiment_model.pt')
                print('Test loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(test_loss_min, np.mean(test_losses)))
                test_loss_min = np.mean(test_losses)

